{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ee040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dfa75",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Pavankalyan/stage0_c_all\"\n",
    "df = load_dataset(data_path, split=\"train\")\n",
    "\n",
    "id_counter = Counter()\n",
    "for batch in df.iter(batch_size=10000):\n",
    "    id_counter.update(batch['id'])\n",
    "    \n",
    "least_common = sorted(id_counter.items(), key=lambda x: x[1])  # ascending sort by count\n",
    "id_to_indices = defaultdict(list)\n",
    "for idx, row in enumerate(df):\n",
    "    id_to_indices[row[\"id\"]].append(idx)\n",
    "val_indices = []\n",
    "for indices in tqdm(id_to_indices.values()):\n",
    "    if len(indices) <= 100:\n",
    "        val_indices.extend(indices)\n",
    "    else:\n",
    "        val_indices.extend(random.sample(indices, 100))\n",
    "\n",
    "val_indices_set = set(val_indices)\n",
    "def is_val(example_idx):\n",
    "    return example_idx in val_indices_set\n",
    "\n",
    "val_dataset = df.select([i for i in range(len(df)) if i in val_indices_set])\n",
    "train_dataset = df.select([i for i in range(len(df)) if i not in val_indices_set])\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.push_to_hub(\"Pavankalyan/stage0_c_all\", token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e91ba",
   "metadata": {},
   "source": [
    "## Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Pavankalyan/stage0_instruct\"\n",
    "df = load_dataset(data_path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3676549",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_counter = Counter()\n",
    "for batch in df.iter(batch_size=10000):\n",
    "    id_counter.update(batch['id'])\n",
    "least_common = sorted(id_counter.items(), key=lambda x: x[1])  # ascending sort by count\n",
    "id_to_indices = defaultdict(list)\n",
    "for idx, row in enumerate(df):\n",
    "    id_to_indices[row[\"id\"]].append(idx)\n",
    "    \n",
    "val_indices = []\n",
    "for indices in tqdm(id_to_indices.values()):\n",
    "    if len(indices) <= 100:\n",
    "        val_indices.extend(indices)\n",
    "    else:\n",
    "        val_indices.extend(random.sample(indices, 100))\n",
    "\n",
    "val_indices_set = set(val_indices)\n",
    "def is_val(example_idx):\n",
    "    return example_idx in val_indices_set\n",
    "\n",
    "val_dataset = df.select([i for i in range(len(df)) if i in val_indices_set])\n",
    "train_dataset = df.select([i for i in range(len(df)) if i not in val_indices_set])\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.push_to_hub(\"Pavankalyan/stage0_instruct\", token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffe6c0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c14e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage=0\n",
    "ds = load_dataset(f\"Pavankalyan/stage{stage}_instruct\", split=\"val\")\n",
    "results = []\n",
    "for i in range(len(ds)):\n",
    "    results.append({\n",
    "        \"instruction\": ds[i][\"instruction\"],\n",
    "        \"response\": ds[i][\"response\"],\n",
    "        \"stage\": ds[i][\"stage\"],\n",
    "        \"age_group\": ds[i][\"age_group\"]\n",
    "    })\n",
    "with open(f\"seed_stage{stage}_instruct.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "prompt = {\n",
    "    \"system\": \"You are a developmental expert rating how well a child's response to a prompt demonstrates age-appropriate reasoning and language for a given developmental stage.\\n\\nYou will receive:\\n- An **instruction** given to the child\\n- The child's **response**\\n- The child's **developmental stage** (0–9)\\n- The child's **age group** (e.g., '0–5', '5–11', '11–14')\\n\\nYour job is to:\\n1. **Rate the response on a scale from 1 to 5**, using the following criteria:\\n   - **5 – Excellent:** The response fully addresses the instruction with clear, developmentally appropriate reasoning and language. It meets expectations for the stage with no major issues.\\n   - **4 – Strong:** Mostly appropriate and coherent; minor gaps in clarity, depth, or completeness.\\n   - **3 – Adequate:** A reasonable attempt that partially addresses the instruction; may be vague, brief, or contain small misunderstandings.\\n   - **2 – Limited:** Weak or underdeveloped response; minimal reasoning or limited relevance to the instruction.\\n   - **1 – Inadequate:** Response is off-topic, confusing, or clearly inappropriate for the stage.\\n\\n2. **Use stage-specific developmental expectations**:\\n   - **Stage 0 (Age 5):** Very simple sentences, concrete ideas, focused on here and now\\n   - **Stages 1–3 (Ages 6–8):** Simple reasoning, some past/future thinking, familiar examples\\n   - **Stages 4–6 (Ages 9–11):** Logical structure, comparisons, abstract or hypothetical reasoning\\n   - **Stages 7–9 (Ages 12–14):** Nuanced reasoning, multi-step thinking, advanced vocabulary\\n\\n3. **Evaluate:**\\n   - Does the child’s response meaningfully address the instruction?\\n   - Is the language and reasoning developmentally appropriate for the stage?\\n   - Is the response authentic and logically consistent?\\n\\n4. **Output Format:**\\nOnly return the following dictionary:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\\nDo not add any other text or formatting. Only return the JSON object.\",\n",
    "    \"user\": \"Evaluate the child's response to the instruction below based on the developmental stage and age group. Return a numerical rating (1–5) and a short explanation.\\n\\nInstruction: {instruction}\\nResponse: {response}\\nStage: {stage}\\nAge group: {age_group}\\n\\n**Output Format:**\\nOnly return the following dictionary:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\\n\"\n",
    "}\n",
    "with open(\"prompt.json\", \"w\") as f:\n",
    "    json.dump(prompt, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = f\"outputs_stage{stage}/instruct\"\n",
    "hf_df = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=os.path.join(res_path, \"*.parquet\"),\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "def parse_json_string(text):\n",
    "    try:\n",
    "        cleaned = text.strip()\n",
    "        cleaned = re.sub(r'^```json\\s*', '', cleaned, flags=re.MULTILINE)\n",
    "        cleaned = re.sub(r'^```', '', cleaned, flags=re.MULTILINE)\n",
    "        cleaned = re.sub(r'```$', '', cleaned, flags=re.MULTILINE)\n",
    "\n",
    "        parsed = json.loads(cleaned)\n",
    "        return {\n",
    "            \"rating\": parsed.get(\"rating\"),\n",
    "            \"explanation\": parsed.get(\"explanation\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        def extract_int(key):\n",
    "            match = re.search(rf'\"{key}\"\\s*:\\s*(\\d+)', cleaned)\n",
    "            return int(match.group(1)) if match else None\n",
    "\n",
    "        rating = extract_int(\"rating\")\n",
    "        return {\n",
    "            \"rating\": rating,\n",
    "            \"explanation\": None\n",
    "        }\n",
    "\n",
    "def flatten_parsed_fields(example):\n",
    "    parsed = parse_json_string(example['answer'])\n",
    "    return {\n",
    "        \"rating\": parsed.get(\"rating\")\n",
    "    }\n",
    "\n",
    "hf_df = hf_df['train'].map(flatten_parsed_fields)\n",
    "\n",
    "def extract_details(row):\n",
    "    seed_data = row['user']\n",
    "    f1 = '''\\n\\nInstruction: '''\n",
    "    f2 = '''\\nResponse: '''\n",
    "    f3 = '''\\nStage: '''\n",
    "    s = seed_data.find(f1)\n",
    "    e = seed_data.find(f2)\n",
    "    m = seed_data.find(f3)\n",
    "    ins = seed_data[s+len(f1):e].strip()\n",
    "    resp = seed_data[e+len(f2):m].strip()\n",
    "    return {\n",
    "            'instruction': ins,\n",
    "            'response' : resp\n",
    "        }\n",
    "    \n",
    "hf_df = hf_df.remove_columns(['batch_uuid', 'embeddings', 'generated_tokens', 'messages', 'metrics', 'num_generated_tokens', 'num_input_tokens', 'params', 'prompt', 'prompt_token_ids', 'request_id', 'system', 'time_taken_llm'])\n",
    "hf_df = hf_df.map(extract_details)\n",
    "ds = ds.to_pandas()\n",
    "hf_df = hf_df.to_pandas()\n",
    "df_merged = ds.merge(hf_df, on=['instruction', 'response'])\n",
    "print(len(df_merged), len(ds_orig))\n",
    "\n",
    "def select_top25(group):\n",
    "    # First get all rating 5s\n",
    "    rating_5 = group[group['rating'] == 5]\n",
    "    if len(rating_5) >= 25:\n",
    "        return rating_5.sample(n=25, random_state=42)\n",
    "    else:\n",
    "        # Get remaining from rating 4\n",
    "        rating_4 = group[group['rating'] == 4]\n",
    "        needed = 25 - len(rating_5)\n",
    "        rating_4_sample = rating_4.sample(n=min(needed, len(rating_4)), random_state=42)\n",
    "        return pd.concat([rating_5, rating_4_sample])\n",
    "\n",
    "selected_df = df_merged.groupby('id', group_keys=False).apply(select_top25)\n",
    "selected_df = selected_df.copy()\n",
    "selected_df['split'] = 'test'\n",
    "df = df_merged.copy()\n",
    "df['split'] = 'val'\n",
    "df.loc[selected_df.index, 'split'] = 'test'\n",
    "print(df['split'].value_counts())\n",
    "df_test = df[df['split']=='test']\n",
    "print(df_test['rating'].value_counts())\n",
    "df_val = df[df['split']=='val']\n",
    "df_val['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=['answer', 'generated_text', 'user', 'split', 'rating'], inplace=True)\n",
    "df_val.drop(columns=['answer', 'generated_text', 'user', 'split', 'rating'], inplace=True)\n",
    "ds = load_dataset(f\"Pavankalyan/stage{stage}_instruct\")\n",
    "df_test = Dataset.from_pandas(df_test, preserve_index=False)\n",
    "df_val = Dataset.from_pandas(df_val, preserve_index=False)\n",
    "df_val.push_to_hub(f\"Pavankalyan/stage{stage}_instruct\", split=\"val\", token=\"\")\n",
    "df_test.push_to_hub(f\"Pavankalyan/stage{stage}_instruct\", split=\"test\", token=\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
